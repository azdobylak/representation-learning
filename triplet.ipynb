{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "session = tf.InteractiveSession(config=config)\n",
    "set_session(session)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_number = 10\n",
    "from random import randint, uniform\n",
    "\n",
    "batch_size = 128\n",
    "fc_sizes = [128, 128]\n",
    "dropouts = [.2, .2]\n",
    "learning_rate = 6e-4\n",
    "margin = .1\n",
    "method = 'embeddings'\n",
    "#method = 'classification'\n",
    "\n",
    "log_dir = \"logdir-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, classes_number)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, classes_number)\n",
    "\n",
    "mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1, 1, 1, 3)\n",
    "std = np.array([0.247, 0.243, 0.261]).reshape(1, 1, 1, 3)\n",
    "\n",
    "x_train = (x_train / 255.0 - mean) / std\n",
    "x_test = (x_test / 255.0 - mean) / std\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "split_index = 10000\n",
    "_validation_percentage = 0.9\n",
    "_dataset_len = x_train.shape[0]\n",
    "split_index = int(_dataset_len * _validation_percentage)\n",
    "\n",
    "indices = numpy.random.permutation(_dataset_len)\n",
    "\n",
    "train_indices = indices[:split_index]\n",
    "validation_indices = indices[split_index:]\n",
    "\n",
    "x, y = x_train, y_train\n",
    "\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_validation = x[validation_indices]\n",
    "y_validation = y[validation_indices]\n",
    "\n",
    "shape_x_train = x_train.shape\n",
    "shape_x_test = x_test.shape\n",
    "shape_x_validation = x_validation.shape\n",
    "shape_y_train = y_train.shape\n",
    "shape_y_test = y_test.shape\n",
    "shape_y_validation = y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset \\\n",
    "    .from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(buffer_size=7000) \\\n",
    "    .batch(batch_size) \\\n",
    "\n",
    "dataset_validation = tf.data.Dataset \\\n",
    "    .from_tensor_slices((x_validation, y_validation)) \\\n",
    "    .batch(batch_size)\n",
    "\n",
    "dataset_test = tf.data.Dataset \\\n",
    "    .from_tensor_slices((x_test, y_test)) \\\n",
    "    .batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adrian/.pyenv/versions/3.6.8/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "train_iterator = dataset_train.make_initializable_iterator()\n",
    "validation_iterator = dataset_validation.make_initializable_iterator()\n",
    "test_iterator = dataset_test.make_initializable_iterator()\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, dataset_train.output_types, dataset_train.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "train_iterator = dataset_train.make_initializable_iterator()\n",
    "validation_iterator = dataset_validation.make_initializable_iterator()\n",
    "test_iterator = dataset_test.make_initializable_iterator()\n",
    "\n",
    "train_handle = session.run(train_iterator.string_handle())\n",
    "validation_handle = session.run(validation_iterator.string_handle())\n",
    "test_handle = session.run(test_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(train_iterator.initializer)\n",
    "session.run(validation_iterator.initializer)\n",
    "session.run(test_iterator.initializer)\n",
    "\n",
    "train_handle = session.run(train_iterator.string_handle())\n",
    "validation_handle = session.run(validation_iterator.string_handle())\n",
    "test_handle = session.run(test_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, x_train, x_test, x_validation, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adrian/.pyenv/versions/3.6.8/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained weights\n",
    "pretrained = tf.keras.applications.VGG16(weights='imagenet', \n",
    "                                         include_top=False, \n",
    "                                         classes=classes_number, \n",
    "                                         input_tensor=next_element[0])\n",
    "                                         #input_shape=(32, 32, 3))\n",
    "\n",
    "for layer in pretrained.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#last_pretrained_layer = pretrained.get_layer('block4_pool')\n",
    "last_pretrained_layer = pretrained\n",
    "\n",
    "fc = last_pretrained_layer.output\n",
    "\n",
    "fc = tf.keras.layers.Dropout(dropouts[0])(fc)\n",
    "fc = tf.keras.layers.Flatten(name='flatten')(fc)\n",
    "fc = tf.keras.layers.Dense(fc_sizes[0], activation='relu', name='fc1')(fc)\n",
    "fc = tf.keras.layers.Dropout(dropouts[1])(fc)\n",
    "fc = tf.keras.layers.Dense(fc_sizes[-1], activation='relu', name='fc2')(fc)\n",
    "\n",
    "embeddings = fc\n",
    "\n",
    "fc = tf.keras.layers.Dropout(0.3)(fc)\n",
    "y_pred = tf.keras.layers.Dense(classes_number)(fc)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = tf.placeholder(\"int32\")\n",
    "x_keys = tf.placeholder(\"float\", [None, fc_sizes[-1]])\n",
    "x_queries = tf.placeholder(\"float\", [None, fc_sizes[-1]])\n",
    "y_keys = tf.placeholder(\"int32\", [None])\n",
    "\n",
    "distance = tf.reduce_sum(tf.abs(tf.subtract(x_keys, tf.expand_dims(x_queries,1))), axis=2)\n",
    "\n",
    "_, indices = tf.nn.top_k(tf.negative(distance), k=k)\n",
    "\n",
    "nearest = tf.gather(y_keys, indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.numpy import mean_average_precision\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def get_embeddings(session, _embeddings, _handle):\n",
    "    embs = []\n",
    "    ys = []\n",
    "    try:\n",
    "        while True:\n",
    "            _embedding, _y = session.run([_embeddings, next_element[1]], feed_dict={handle: _handle})\n",
    "            embs.append(_embedding)\n",
    "            ys.append(_y)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    return np.concatenate(embs), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def knn_classes(embeddings_train, embeddings_target, y_train, items_per_step=20):\n",
    "    knn_outs = []\n",
    "    max_step = _y_target.shape[0] // items_per_step\n",
    "    \n",
    "    keys_train = np.argmax(y_train, axis=1)\n",
    "    \n",
    "    for i in range(max_step):\n",
    "        clear_output(wait=True)\n",
    "        res = session.run(nearest, feed_dict={\n",
    "            x_keys: embeddings_train,\n",
    "            y_keys: keys_train,\n",
    "            x_queries: embeddings_target[i*items_per_step:(i+1)*items_per_step],\n",
    "            k: y_train.shape[0]})\n",
    "        knn_outs.append(res)\n",
    "        display('KNN, step[{}/{}]'.format(i + 1, max_step))\n",
    "        \n",
    "    knn_res = np.concatenate(knn_outs)\n",
    "    \n",
    "    return knn_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "time_str = time.strftime(\"%m-%d__%H:%M:%S\", time.gmtime())\n",
    "tensorboard_dir = f\"{time_str}/adam{learning_rate}_fc{fc_sizes}_dropouts{dropouts}_batch{batch_size}_margin{margin}\"\n",
    "\n",
    "writer_train = tf.summary.FileWriter(\n",
    "    logdir=os.path.join(log_dir, \"train\", tensorboard_dir))\n",
    "\n",
    "writer_validation = tf.summary.FileWriter(\n",
    "    logdir=os.path.join(log_dir, \"validate\", tensorboard_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet loss\n",
      "WARNING:tensorflow:From /home/adrian/.pyenv/versions/3.6.8/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from loss.triplet import (triplet_loss_batch_all,\n",
    "                          triplet_loss_batch_hard,\n",
    "                          triplet_loss_batch_hard_negative)\n",
    "\n",
    "if method == 'embeddings':\n",
    "    print('Triplet loss')\n",
    "    labels = tf.argmax(next_element[1], axis=1)\n",
    "    loss = triplet_loss_batch_all(embeddings, labels, margin=margin)\n",
    "elif method == 'classification':\n",
    "    print('cross_entropy')\n",
    "    labels = next_element[1]\n",
    "    loss = tf.losses.softmax_cross_entropy(labels, y_pred)\n",
    "else:\n",
    "    raise ValueError(f\"{method} has incorrect value\")\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0\n",
    "loss_accum = 0\n",
    "epoch = 0\n",
    "summary_op = tf.summary.merge_all()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KNN, step[287/500]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metrics.numpy import mean_average_precision\n",
    "\n",
    "\n",
    "last_train_batch = batch\n",
    "last_mAP = 0.0\n",
    "\n",
    "while True:\n",
    "    # train\n",
    "    batch = last_train_batch\n",
    "    try:\n",
    "        while True:\n",
    "            batch += 1\n",
    "            summary, _, loss_val = session.run(\n",
    "                [summary_op, optimizer, loss],\n",
    "                feed_dict={handle: train_handle}\n",
    "            )\n",
    "            writer_train.add_summary(summary, batch)\n",
    "            writer_train.flush()\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        session.run(train_iterator.initializer)\n",
    "        last_train_batch = batch\n",
    "    #validate\n",
    "    try:\n",
    "        while True:\n",
    "            batch += 1\n",
    "            summary, loss_val = session.run(\n",
    "                [summary_op, loss],\n",
    "                feed_dict={handle: validation_handle}\n",
    "            )\n",
    "            writer_validation.add_summary(summary, batch)\n",
    "            writer_validation.flush()\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        session.run(validation_iterator.initializer)\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "    if epoch % 4 == 0:    \n",
    "        session.run(train_iterator.initializer)\n",
    "        session.run(validation_iterator.initializer)\n",
    "\n",
    "        embeddings_train, _y_train = get_embeddings(session, embeddings, train_handle)\n",
    "        embeddings_target, _y_target = get_embeddings(session, embeddings, validation_handle)\n",
    "\n",
    "        knn_res = knn_classes(embeddings_train, embeddings_target, _y_train, items_per_step=10)\n",
    "\n",
    "        mAP = mean_average_precision(knn_res, np.argmax(_y_target, axis=1))\n",
    "\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='validation/mAP', simple_value=mAP)\n",
    "        writer_validation.add_summary(summary, batch)\n",
    "        writer_validation.flush()\n",
    "        \n",
    "        print(mAP)\n",
    "        \n",
    "        if mAP < last_mAP:\n",
    "            print('Finished')\n",
    "            break\n",
    "            \n",
    "        last_mAP = mAP\n",
    "        \n",
    "\n",
    "session.run(train_iterator.initializer)\n",
    "session.run(test_iterator.initializer)\n",
    "\n",
    "embeddings_train, _y_train = get_embeddings(session, embeddings, train_handle)\n",
    "embeddings_target, _y_target = get_embeddings(session, embeddings, test_handle)\n",
    "\n",
    "knn_res = knn_classes(embeddings_train, embeddings_target, _y_train, items_per_step=10)\n",
    "\n",
    "mAP = mean_average_precision(knn_res, np.argmax(_y_target, axis=1))\n",
    "print(f\"test mAP={mAP}\")\n",
    "\n",
    "summary = tf.Summary()\n",
    "summary.value.add(tag='test/mAP', simple_value=mAP)\n",
    "writer_validation.add_summary(summary, batch)\n",
    "writer_validation.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore variables from disk.\n",
    "# saver.restore(session, f\"saved_models/{tensorboard_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
